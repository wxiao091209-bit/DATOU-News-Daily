#!/usr/bin/env python3
import os
import re
import json
import html
import random
from datetime import datetime, timedelta
from urllib.parse import urljoin
import feedparser
import requests
from bs4 import BeautifulSoup

# å°è¯•å¯¼å…¥ç¿»è¯‘åº“ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å¢å¼ºå­—å…¸
try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
    translator = GoogleTranslator(source='auto', target='zh-CN')
except ImportError:
    TRANSLATOR_AVAILABLE = False
    print("æ³¨æ„ï¼šæœªå®‰è£… deep_translatorï¼Œä½¿ç”¨æœ¬åœ°å­—å…¸ç¿»è¯‘")

def smart_translate(text, max_chars=150):
    """æ™ºèƒ½ç¿»è¯‘ï¼šå…ˆå°è¯•APIï¼Œå¤±è´¥ç”¨å­—å…¸"""
    if not text:
        return "æš‚æ— å†…å®¹"
    
    # å¦‚æœå·²æœ‰è¶³å¤Ÿä¸­æ–‡ï¼Œç›´æ¥è¿”å›
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    if chinese_chars > len(text) * 0.4:
        return text[:max_chars]
    
    # æ–¹æ³•1ï¼šä½¿ç”¨Google Translate APIï¼ˆå…è´¹ï¼‰
    if TRANSLATOR_AVAILABLE:
        try:
            # åˆ†æ®µç¿»è¯‘ï¼ˆAPIæœ‰é•¿åº¦é™åˆ¶ï¼‰
            if len(text) > 4000:
                text = text[:4000]
            translated = translator.translate(text)
            return translated[:max_chars]
        except Exception as e:
            print(f"APIç¿»è¯‘å¤±è´¥: {e}ï¼Œä½¿ç”¨å­—å…¸ç¿»è¯‘")
    
    # æ–¹æ³•2ï¼šå¢å¼ºå­—å…¸ç¿»è¯‘ï¼ˆå¤‡ç”¨ï¼‰
    return enhanced_dict_translate(text, max_chars)

def enhanced_dict_translate(text, max_chars=150):
    """å¢å¼ºç‰ˆå­—å…¸ç¿»è¯‘ï¼Œè¦†ç›–æ›´å¤šè¯æ±‡"""
    if not text:
        return "æš‚æ— å†…å®¹"
    
    # æ‰©å±•çš„ç¿»è¯‘å­—å…¸
    translations = [
        # åŸºç¡€è¯æ±‡ï¼ˆæŒ‰é•¿åº¦é™åºï¼‰
        ("artificial intelligence", "äººå·¥æ™ºèƒ½"),
        ("machine learning", "æœºå™¨å­¦ä¹ "),
        ("large language model", "å¤§è¯­è¨€æ¨¡å‹"),
        ("command center", "æŒ‡æŒ¥ä¸­å¿ƒ"),
        ("software development", "è½¯ä»¶å¼€å‘"),
        ("enterprise data", "ä¼ä¸šçº§æ•°æ®"),
        ("frontier intelligence", "å‰æ²¿æ™ºèƒ½"),
        ("AI agents", "AIæ™ºèƒ½ä½“"),
        ("AI agent", "AIæ™ºèƒ½ä½“"),
        ("data agent", "æ•°æ®æ™ºèƒ½ä½“"),
        ("in-house", "è‡ªç ”"),
        ("most valuable", "æœ€æœ‰ä»·å€¼çš„"),
        ("private company", "ç§è¥å…¬å¸"),
        ("weekly newsletter", "æ¯å‘¨é€šè®¯"),
        
        # å…¬å¸/äº§å“
        ("OpenAI", "OpenAI"),
        ("Anthropic", "Anthropic"),
        ("Claude", "Claude"),
        ("GPT-5", "GPT-5"),
        ("GPT-4", "GPT-4"),
        ("GPT", "GPT"),
        ("Gemini", "Gemini"),
        ("Google", "è°·æ­Œ"),
        ("Meta", "Meta"),
        ("NVIDIA", "è‹±ä¼Ÿè¾¾"),
        ("Microsoft", "å¾®è½¯"),
        ("Amazon", "äºšé©¬é€Š"),
        ("Snowflake", "Snowflake"),
        ("TechCrunch", "TechCrunch"),
        ("Musk", "é©¬æ–¯å…‹"),
        ("xAI", "xAI"),
        ("Codex", "Codex"),
        
        # åŠ¨è¯
        ("partner", "åˆä½œ"),
        ("partnership", "åˆä½œ"),
        ("introducing", "æ¨å‡º"),
        ("introduces", "æ¨å‡º"),
        ("announce", "å®£å¸ƒ"),
        ("announcing", "å®£å¸ƒ"),
        ("launch", "å‘å¸ƒ"),
        ("launches", "å‘å¸ƒ"),
        ("release", "å‘å¸ƒ"),
        ("update", "æ›´æ–°"),
        ("bring", "å¼•å…¥"),
        ("enable", "ä½¿èƒ½å¤Ÿ"),
        ("enabling", "ä½¿"),
        ("creates", "åˆ›å»º"),
        ("create", "åˆ›å»º"),
        ("built", "æ„å»º"),
        ("build", "æ„å»º"),
        ("appeared", "å‘è¡¨äº"),
        ("appear", "å‡ºç°"),
        ("paves", "é“ºå¹³"),
        ("pave", "é“ºå¹³"),
        ("prove", "è¯æ˜"),
        ("try", "å°è¯•"),
        ("get", "è·å–"),
        
        # åè¯
        ("merger", "åˆå¹¶"),
        ("agreement", "åè®®"),
        ("investment", "æŠ•èµ„"),
        ("funding", "èèµ„"),
        ("company", "å…¬å¸"),
        ("business", "ä¸šåŠ¡"),
        ("story", "æŠ¥é“"),
        ("algorithm", "ç®—æ³•"),
        ("newsletter", "é€šè®¯"),
        ("inbox", "æ”¶ä»¶ç®±"),
        ("way", "é“è·¯/æ–¹å¼"),
        ("world", "ä¸–ç•Œ"),
        ("insight", "æ´å¯Ÿ"),
        ("intelligence", "æ™ºèƒ½"),
        ("data", "æ•°æ®"),
        ("chip", "èŠ¯ç‰‡"),
        ("robotics", "æœºå™¨äºº"),
        ("infrastructure", "åŸºç¡€è®¾æ–½"),
        
        # å½¢å®¹è¯/å‰¯è¯
        ("valuable", "æœ‰ä»·å€¼çš„"),
        ("private", "ç§æœ‰çš„"),
        ("weekly", "æ¯å‘¨çš„"),
        ("most", "æœ€"),
        ("more", "æ›´å¤š"),
        ("useful", "æœ‰ç”¨çš„"),
        ("original", "åŸåˆ›çš„"),
        ("directly", "ç›´æ¥"),
        ("massive", "æµ·é‡çš„"),
        ("reliable", "å¯é çš„"),
        ("long-running", "é•¿æ—¶é—´è¿è¡Œçš„"),
        ("parallel", "å¹¶è¡Œçš„"),
        ("multiple", "å¤šä¸ª"),
        
        # ä»‹è¯/å† è¯/è¿è¯ï¼ˆå°å†™åŒ¹é…ï¼‰
        (" the ", " "),
        (" and ", "å’Œ"),
        (" in ", "åœ¨"),
        (" a ", "ä¸€ä¸ª"),
        (" an ", "ä¸€ä¸ª"),
        (" to ", "æ¥"),
        (" of ", "çš„"),
        (" for ", "ç”¨äº"),
        (" with ", "ä¸"),
        (" by ", "é€šè¿‡"),
        (" from ", "æ¥è‡ª"),
        (" into ", "è¿›å…¥"),
        (" on ", "åœ¨"),
        (" at ", "åœ¨"),
    ]
    
    result = text
    for en, cn in sorted(translations, key=lambda x: len(x[0]), reverse=True):
        # ä¸åŒºåˆ†å¤§å°å†™æ›¿æ¢ï¼Œä½†ä¿ç•™åŸå¤§å°å†™ç”¨äºåˆ¤æ–­
        result = re.sub(r'\b' + re.escape(en) + r'\b', cn, result, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
    result = re.sub(r'\s+', ' ', result).strip()
    result = re.sub(r' ([ï¼Œã€‚ã€ï¼›ï¼šï¼Ÿï¼])', r'\1', result)  # ç§»é™¤æ ‡ç‚¹å‰ç©ºæ ¼
    
    # å¦‚æœç¿»è¯‘åè¿˜æ˜¯è‹±æ–‡ä¸ºä¸»ï¼Œæ ‡è®°ä¸º[åŸæ–‡]
    chinese_count = len([c for c in result if '\u4e00' <= c <= '\u9fff'])
    if chinese_count < len(result) * 0.3:
        return f"[æµ·å¤–èµ„è®¯] {text[:max_chars-10]}"
    
    return result[:max_chars]

# ä¸€æ‰‹ä¿¡æºé…ç½®
SOURCES = {
    "bigModel": [
        {"name": "OpenAI Blog", "url": "https://openai.com/blog/rss.xml", "type": "rss"},
        {"name": "Anthropic News", "url": "https://www.anthropic.com/news/rss.xml", "type": "rss"},
        {"name": "Google DeepMind", "url": "https://deepmind.google/discover/feed/", "type": "rss"},
        {"name": "Meta AI Blog", "url": "https://ai.meta.com/blog/rss/", "type": "rss"},
        {"name": "Hugging Face Papers", "url": "https://huggingface.co/api/papers?limit=15", "type": "hf_api"},
        {"name": "Mistral AI", "url": "https://mistral.ai/news", "type": "html", "selector": "article h2 a, article h3 a"},
        {"name": "xAI", "url": "https://x.ai/news", "type": "html", "selector": "a[href*='/news/']"},
    ],
    "hardware": [
        {"name": "NVIDIA Blog AI", "url": "https://blogs.nvidia.com/blog/category/artificial-intelligence/feed/", "type": "rss"},
        {"name": "NVIDIA Robotics", "url": "https://blogs.nvidia.com/blog/category/robotics/feed/", "type": "rss"},
    ],
    "investment": [
        {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed/", "type": "rss"},
        {"name": "TechCrunch Funding", "url": "https://techcrunch.com/category/venture/feed/", "type": "rss"},
    ],
    "global": [
        {"name": "MIT Tech Review AI", "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed", "type": "rss"},
        {"name": "The Verge AI", "url": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml", "type": "rss"},
    ],
    "industry": [
        {"name": "Google AI Blog", "url": "https://ai.googleblog.com/feeds/posts/default", "type": "rss"},
        {"name": "Microsoft AI", "url": "https://blogs.microsoft.com/ai/feed/", "type": "rss"},
    ],
    "product": [
        {"name": "OpenAI Product", "url": "https://openai.com/blog/rss.xml", "type": "rss"},
        {"name": "Anthropic Product", "url": "https://www.anthropic.com/news/rss.xml", "type": "rss"},
    ]
}

CATEGORY_META = {
    "bigModel": {
        "title": "å¤§æ¨¡å‹",
        "desc": "GPT-5ã€Claude 4ã€Gemini Ultra ç­‰å‰æ²¿å¤§æ¨¡å‹æŠ€æœ¯çªç ´ä¸å•†ä¸šåŒ–è¿›å±•è¿½è¸ª",
        "icon": "M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"
    },
    "hardware": {
        "title": "AI ç¡¬ä»¶",
        "desc": "ç®—åŠ›èŠ¯ç‰‡ã€æœºå™¨äººã€ç«¯ä¾§è®¾å¤‡ã€AI Phone ç­‰ç¡¬ä»¶è½½ä½“æŠ€æœ¯é©æ–°ä¸äº§ä¸šé“¾åŠ¨å‘",
        "icon": "M9 3v2m6-2v2M9 19v2m6-2v2M5 9H3m2 6H3m18-6h-2m2 6h-2M7 19h10a2 2 0 002-2V7a2 2 0 00-2-2H7a2 2 0 00-2 2v10a2 2 0 002 2zM9 9h6v6H9V9z"
    },
    "global": {
        "title": "å‡ºæµ·åŠ¨æ€",
        "desc": "ä¸­å›½ AI ä¼ä¸šå…¨çƒåŒ–å¸ƒå±€ã€æµ·å¤–ç›‘ç®¡æ”¿ç­–ã€è·¨å¢ƒæŠ•èèµ„ä¸æœ¬åœ°åŒ–æˆ˜ç•¥åˆ†æ",
        "icon": "M3.055 11H5a2 2 0 012 2v1a2 2 0 002 2 2 2 0 012 2v2.945M8 3.935V5.5A2.5 2.5 0 0010.5 8h.5a2 2 0 012 2 2 2 0 104 0 2 2 0 012-2h1.064M15 20.488V18a2 2 0 012-2h3.064M21 12a9 9 0 11-18 0 9 9 0 0118 0z"
    },
    "investment": {
        "title": "æŠ•èèµ„",
        "desc": "ä¸€çº§å¸‚åœºèèµ„é€Ÿé€’ã€ç‹¬è§’å…½ä¼°å€¼å˜åŠ¨ã€IPO åŠ¨æ€ä¸èµ„æœ¬é£å‘è§£è¯»",
        "icon": "M12 8c-1.657 0-3 .895-3 2s1.343 2 3 2 3 .895 3 2-1.343 2-3 2m0-8c1.11 0 2.08.402 2.599 1M12 8V7m0 1v8m0 0v1m0-1c-1.11 0-2.08-.402-2.599-1M21 12a9 9 0 11-18 0 9 9 0 0118 0z"
    },
    "industry": {
        "title": "äº§ä¸šè§‚å¯Ÿ",
        "desc": "è¡Œä¸šæ”¿ç­–è§£è¯»ã€ç«äº‰æ ¼å±€åˆ†æã€æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹ä¸å•†ä¸šæ¨¡å¼æ¼”è¿›ç ”ç©¶",
        "icon": "M19 21V5a2 2 0 00-2-2H7a2 2 0 00-2 2v16m14 0h2m-2 0h-5m-9 0H3m2 0h5M9 7h1m-1 4h1m4-4h1m-1 4h1m-5 10v-5a1 1 0 011-1h2a1 1 0 011 1v5m-4 0h4"
    },
    "product": {
        "title": "äº§å“å¿«è®¯",
        "desc": "AI åº”ç”¨æ–°å“å‘å¸ƒã€åŠŸèƒ½æ›´æ–°ã€ç”¨æˆ·ä½“éªŒä¼˜åŒ–ä¸å¸‚åœºåŒ–ç­–ç•¥è¿½è¸ªæŠ¥é“",
        "icon": "M13 10V3L4 14h7v7l9-11h-7z"
    }
}

def fetch_rss(url, name):
    """æŠ“å–RSS"""
    try:
        print(f"Fetching: {name}")
        feed = feedparser.parse(url)
        entries = []
        for entry in feed.entries[:5]:
            summary = entry.get('summary', entry.get('description', ''))
            clean_summary = re.sub(r'<[^>]+>', '', summary)
            
            # ç¿»è¯‘å¤„ç†
            title_cn = smart_translate(entry.title, 100)
            summary_cn = smart_translate(clean_summary, 150)
            
            entries.append({
                "title": title_cn,
                "link": entry.link,
                "date": entry.get('published', ''),
                "summary": summary_cn,
                "source": name,
                "content": f"<p>{summary_cn}</p><p><a href='{entry.link}' target='_blank'>æŸ¥çœ‹åŸæ–‡ï¼š{name}</a></p>"
            })
        print(f"  âœ“ {len(entries)}æ¡")
        return entries
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []

def fetch_hf_papers():
    """æŠ“å–Hugging Faceè®ºæ–‡"""
    try:
        print("Fetching: Hugging Face Papers")
        resp = requests.get("https://huggingface.co/api/papers?limit=10", timeout=15)
        papers = resp.json()
        entries = []
        for p in papers:
            title = p.get('title', '')
            summary = p.get('summary', '')
            paper_id = p.get('id', '')
            
            if title:
                entries.append({
                    "title": f"[è®ºæ–‡] {smart_translate(title, 100)}",
                    "link": f"https://huggingface.co/papers/{paper_id}",
                    "date": p.get('publishedAt', ''),
                    "summary": smart_translate(summary, 150) if summary else "æœ€æ–°AIç ”ç©¶è®ºæ–‡",
                    "source": "Hugging Face",
                    "content": f"<p>{smart_translate(summary, 200)}</p><p><a href='https://huggingface.co/papers/{paper_id}' target='_blank'>æŸ¥çœ‹è®ºæ–‡</a></p>"
                })
        print(f"  âœ“ {len(entries)}æ¡")
        return entries
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []

def fetch_html_list(url, name, selector):
    """æŠ“å–HTMLåˆ—è¡¨"""
    try:
        print(f"Fetching HTML: {name}")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, 'html.parser')
        entries = []
        
        links = soup.select(selector)[:3]
        for link in links:
            href = link.get('href', '')
            if href and not href.startswith('http'):
                href = urljoin(url, href)
            title = link.get_text(strip=True)
            if title and 10 < len(title) < 150:
                entries.append({
                    "title": smart_translate(title, 100),
                    "link": href,
                    "date": datetime.now().isoformat(),
                    "summary": f"{name}æœ€æ–°åŠ¨æ€",
                    "source": name,
                    "content": f"<p>{name}å‘å¸ƒæ›´æ–°</p><p><a href='{href}' target='_blank'>æŸ¥çœ‹åŸæ–‡</a></p>"
                })
        print(f"  âœ“ {len(entries)}æ¡")
        return entries
    except Exception as e:
        print(f"  âœ— Error: {e}")
        return []

def estimate_read_time(text):
    words = len(text) / 2
    minutes = max(1, round(words / 300))
    return f"{minutes} åˆ†é’Ÿ"

def build_content_database():
    """æ„å»ºæ•°æ®åº“"""
    print("\n" + "="*50)
    database = {"summaries": [[]], "categories": {}}
    all_articles_by_source = {}
    
    # æŠ“å–æ•°æ®
    for cat_key, sources in SOURCES.items():
        for source in sources:
            try:
                if source['type'] == 'rss':
                    entries = fetch_rss(source['url'], source['name'])
                elif source['type'] == 'hf_api':
                    entries = fetch_hf_papers()
                elif source['type'] == 'html':
                    entries = fetch_html_list(source['url'], source['name'], source['selector'])
                else:
                    entries = []
                
                if entries:
                    all_articles_by_source[source['name']] = entries
                if cat_key not in database["categories"]:
                    database["categories"][cat_key] = []
                database["categories"][cat_key].extend(entries)
            except Exception as e:
                print(f"Error: {e}")
    
    # å¤„ç†åˆ†ç±»æ•°æ®
    for cat_key in CATEGORY_META.keys():
        articles = database["categories"].get(cat_key, [])
        seen = set()
        unique = []
        for a in articles:
            if a['link'] not in seen:
                seen.add(a['link'])
                a['readTime'] = estimate_read_time(a.get('summary', ''))
                unique.append(a)
        
        database["categories"][cat_key] = {
            "title": CATEGORY_META[cat_key]["title"],
            "desc": CATEGORY_META[cat_key]["desc"],
            "icon": CATEGORY_META[cat_key]["icon"],
            "articles": unique[:8]
        }
        print(f"{cat_key}: {len(unique[:8])}ç¯‡")
    
    # é€‰æ‹©æ‘˜è¦ï¼ˆå¼ºåˆ¶å¤šæ ·æ€§ï¼‰
    summaries = []
    priority = ["OpenAI Blog", "Anthropic News", "Google DeepMind", "Meta AI Blog", 
                "Hugging Face", "NVIDIA Blog AI", "TechCrunch AI", "MIT Tech Review AI"]
    
    for src in priority:
        if src in all_articles_by_source and all_articles_by_source[src]:
            article = all_articles_by_source[src][0]
            summaries.append({
                "text": article['summary'][:120] + "..." if len(article['summary']) > 120 else article['summary'],
                "source": article['source'],
                "url": article['link']
            })
            print(f"æ‘˜è¦æ¥æº: {src}")
            if len(summaries) >= 3:
                break
    
    database["summaries"] = [summaries[:3]]
    return database

def update_html_file():
    """æ›´æ–°HTML"""
    try:
        with open('index.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        new_db = build_content_database()
        json_str = json.dumps(new_db, ensure_ascii=False, indent=8)
        
        pattern = r'const contentDatabase = \{.*?\};'
        replacement = f'const contentDatabase = {json_str};'
        new_html = re.sub(pattern, replacement, html_content, flags=re.DOTALL)
        
        if new_html == html_content:
            lines = html_content.split('\n')
            for i, line in enumerate(lines):
                if 'const contentDatabase = {' in line:
                    start = i
                    brace = 1
                    j = i + 1
                    while j < len(lines) and brace > 0:
                        brace += lines[j].count('{') - lines[j].count('}')
                        if brace == 0:
                            new_lines = lines[:start] + [f'const contentDatabase = {json_str};'] + lines[j+1:]
                            new_html = '\n'.join(new_lines)
                            break
                        j += 1
                    break
        
        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(new_html)
        
        total = sum(len(c['articles']) for c in new_db['categories'].values())
        print(f"\nâœ… å®Œæˆï¼å…±{total}ç¯‡æ–‡ç« ï¼Œ{len(new_db['summaries'][0])}æ¡æ‘˜è¦")
        
    except Exception as e:
        print(f"Error: {e}")
        raise

if __name__ == '__main__':
    print("ğŸ¤– DATOU AI News - æ™ºèƒ½ç¿»è¯‘ç‰ˆ")
    print("=" * 50)
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    if TRANSLATOR_AVAILABLE:
        print("ğŸŒ ä½¿ç”¨Google Translate API")
    else:
        print("ğŸ“š ä½¿ç”¨æœ¬åœ°å­—å…¸ç¿»è¯‘")
    print("=" * 50)
    update_html_file()
