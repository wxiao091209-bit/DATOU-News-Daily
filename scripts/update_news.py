#!/usr/bin/env python3
import os
import re
import json
import html
import random
from datetime import datetime, timedelta
from urllib.parse import urljoin
import feedparser
import requests
from bs4 import BeautifulSoup

def force_translate(text):
    """å¼ºåˆ¶ç¿»è¯‘è‹±æ–‡æ ‡é¢˜/æ‘˜è¦ä¸ºä¸­æ–‡"""
    if not text:
        return "æš‚æ— å†…å®¹"
    
    # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿå¤šä¸­æ–‡ï¼Œç›´æ¥è¿”å›
    chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
    if chinese_chars > len(text) * 0.3:
        return text
    
    # è‹±ä¸­æ˜ å°„è¡¨ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œé¿å…çŸ­è¯è¦†ç›–ï¼‰
    translations = [
        # å…¬å¸å’Œäº§å“
        ("OpenAI", "OpenAI"),
        ("Anthropic", "Anthropic"),
        ("Claude", "Claude"),
        ("GPT-5", "GPT-5"),
        ("GPT-4", "GPT-4"),
        ("GPT", "GPT"),
        ("Gemini", "Gemini"),
        ("Google", "è°·æ­Œ"),
        ("Meta", "Meta"),
        ("NVIDIA", "è‹±ä¼Ÿè¾¾"),
        ("Microsoft", "å¾®è½¯"),
        ("Amazon", "äºšé©¬é€Š"),
        ("Snowflake", "Snowflake"),
        ("xAI", "xAI"),
        ("Mistral", "Mistral"),
        ("Hugging Face", "Hugging Face"),
        
        # æŠ€æœ¯å’Œäº§å“
        ("Codex", "Codex"),
        ("ChatGPT", "ChatGPT"),
        ("AI agent", "AIæ™ºèƒ½ä½“"),
        ("AI agents", "AIæ™ºèƒ½ä½“"),
        ("artificial intelligence", "äººå·¥æ™ºèƒ½"),
        ("machine learning", "æœºå™¨å­¦ä¹ "),
        ("large language model", "å¤§è¯­è¨€æ¨¡å‹"),
        ("LLM", "å¤§æ¨¡å‹"),
        ("multimodal", "å¤šæ¨¡æ€"),
        ("infrastructure", "åŸºç¡€è®¾æ–½"),
        ("enterprise", "ä¼ä¸šçº§"),
        ("dataset", "æ•°æ®é›†"),
        ("training", "è®­ç»ƒ"),
        ("inference", "æ¨ç†"),
        ("chip", "èŠ¯ç‰‡"),
        ("GPU", "GPU"),
        ("robotics", "æœºå™¨äººæŠ€æœ¯"),
        
        # åŠ¨ä½œå’Œå•†ä¸š
        ("partnership", "åˆä½œ"),
        ("partner", "åˆä½œ"),
        ("agreement", "åè®®"),
        ("investment", "æŠ•èµ„"),
        ("funding", "èèµ„"),
        ("billion", "åäº¿ç¾å…ƒ"),
        ("million", "ç™¾ä¸‡ç¾å…ƒ"),
        ("launch", "å‘å¸ƒ"),
        ("introducing", "æ¨å‡º"),
        ("announce", "å®£å¸ƒ"),
        ("release", "å‘å¸ƒ"),
        ("update", "æ›´æ–°"),
        ("available", "ä¸Šçº¿"),
        ("built", "æ„å»º"),
        ("bring", "å¼•å…¥"),
        
        # æè¿°è¯
        ("frontier", "å‰æ²¿"),
        ("intelligence", "æ™ºèƒ½"),
        ("command center", "æŒ‡æŒ¥ä¸­å¿ƒ"),
        ("software development", "è½¯ä»¶å¼€å‘"),
        ("multiple", "å¤š"),
        ("parallel", "å¹¶è¡Œ"),
        ("workflows", "å·¥ä½œæµ"),
        ("long-running", "é•¿æ—¶é—´è¿è¡Œ"),
        ("reliable", "å¯é çš„"),
        ("insights", "æ´å¯Ÿ"),
        ("reason", "æ¨ç†"),
        ("memory", "è®°å¿†"),
        ("massive", "æµ·é‡"),
    ]
    
    # ç¿»è¯‘å¤„ç†
    result = text
    for en, cn in sorted(translations, key=lambda x: len(x[0]), reverse=True):
        result = re.sub(r'\b' + re.escape(en) + r'\b', cn, result, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    result = re.sub(r'\s+', ' ', result).strip()
    
    return result

# ä¸€æ‰‹ä¿¡æºé…ç½®
SOURCES = {
    "bigModel": [
        {"name": "OpenAI Blog", "url": "https://openai.com/blog/rss.xml", "type": "rss"},
        {"name": "Anthropic News", "url": "https://www.anthropic.com/news/rss.xml", "type": "rss"},
        {"name": "Google DeepMind", "url": "https://deepmind.google/discover/feed/", "type": "rss"},
        {"name": "Meta AI Blog", "url": "https://ai.meta.com/blog/rss/", "type": "rss"},
        {"name": "Hugging Face Papers", "url": "https://huggingface.co/api/papers?limit=15", "type": "hf_api"},
        {"name": "Mistral AI", "url": "https://mistral.ai/news", "type": "html", "selector": "article h2 a, article h3 a"},
        {"name": "xAI", "url": "https://x.ai/news", "type": "html", "selector": "a[href*='/news/']"},
    ],
    "hardware": [
        {"name": "NVIDIA Blog AI", "url": "https://blogs.nvidia.com/blog/category/artificial-intelligence/feed/", "type": "rss"},
        {"name": "NVIDIA Robotics", "url": "https://blogs.nvidia.com/blog/category/robotics/feed/", "type": "rss"},
    ],
    "investment": [
        {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed/", "type": "rss"},
        {"name": "TechCrunch Funding", "url": "https://techcrunch.com/category/venture/feed/", "type": "rss"},
    ],
    "global": [
        {"name": "MIT Tech Review AI", "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed", "type": "rss"},
        {"name": "The Verge AI", "url": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml", "type": "rss"},
    ],
    "industry": [
        {"name": "Google AI Blog", "url": "https://ai.googleblog.com/feeds/posts/default", "type": "rss"},
        {"name": "Microsoft AI", "url": "https://blogs.microsoft.com/ai/feed/", "type": "rss"},
    ],
    "product": [
        {"name": "OpenAI Product", "url": "https://openai.com/blog/rss.xml", "type": "rss"},
        {"name": "Anthropic Product", "url": "https://www.anthropic.com/news/rss.xml", "type": "rss"},
    ]
}

CATEGORY_META = {
    "bigModel": {
        "title": "å¤§æ¨¡å‹",
        "desc": "GPT-5ã€Claude 4ã€Gemini Ultra ç­‰å‰æ²¿å¤§æ¨¡å‹æŠ€æœ¯çªç ´ä¸å•†ä¸šåŒ–è¿›å±•è¿½è¸ª",
        "icon": "M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"
    },
    "hardware": {
        "title": "AI ç¡¬ä»¶",
        "desc": "ç®—åŠ›èŠ¯ç‰‡ã€æœºå™¨äººã€ç«¯ä¾§è®¾å¤‡ã€AI Phone ç­‰ç¡¬ä»¶è½½ä½“æŠ€æœ¯é©æ–°ä¸äº§ä¸šé“¾åŠ¨å‘",
        "icon": "M9 3v2m6-2v2M9 19v2m6-2v2M5 9H3m2 6H3m18-6h-2m2 6h-2M7 19h10a2 2 0 002-2V7a2 2 0 00-2-2H7a2 2 0 00-2 2v10a2 2 0 002 2zM9 9h6v6H9V9z"
    },
    "global": {
        "title": "å‡ºæµ·åŠ¨æ€",
        "desc": "ä¸­å›½ AI ä¼ä¸šå…¨çƒåŒ–å¸ƒå±€ã€æµ·å¤–ç›‘ç®¡æ”¿ç­–ã€è·¨å¢ƒæŠ•èèµ„ä¸æœ¬åœ°åŒ–æˆ˜ç•¥åˆ†æ",
        "icon": "M3.055 11H5a2 2 0 012 2v1a2 2 0 002 2 2 2 0 012 2v2.945M8 3.935V5.5A2.5 2.5 0 0010.5 8h.5a2 2 0 012 2 2 2 0 104 0 2 2 0 012-2h1.064M15 20.488V18a2 2 0 012-2h3.064M21 12a9 9 0 11-18 0 9 9 0 0118 0z"
    },
    "investment": {
        "title": "æŠ•èèµ„",
        "desc": "ä¸€çº§å¸‚åœºèèµ„é€Ÿé€’ã€ç‹¬è§’å…½ä¼°å€¼å˜åŠ¨ã€IPO åŠ¨æ€ä¸èµ„æœ¬é£å‘è§£è¯»",
        "icon": "M12 8c-1.657 0-3 .895-3 2s1.343 2 3 2 3 .895 3 2-1.343 2-3 2m0-8c1.11 0 2.08.402 2.599 1M12 8V7m0 1v8m0 0v1m0-1c-1.11 0-2.08-.402-2.599-1M21 12a9 9 0 11-18 0 9 9 0 0118 0z"
    },
    "industry": {
        "title": "äº§ä¸šè§‚å¯Ÿ",
        "desc": "è¡Œä¸šæ”¿ç­–è§£è¯»ã€ç«äº‰æ ¼å±€åˆ†æã€æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹ä¸å•†ä¸šæ¨¡å¼æ¼”è¿›ç ”ç©¶",
        "icon": "M19 21V5a2 2 0 00-2-2H7a2 2 0 00-2 2v16m14 0h2m-2 0h-5m-9 0H3m2 0h5M9 7h1m-1 4h1m4-4h1m-1 4h1m-5 10v-5a1 1 0 011-1h2a1 1 0 011 1v5m-4 0h4"
    },
    "product": {
        "title": "äº§å“å¿«è®¯",
        "desc": "AI åº”ç”¨æ–°å“å‘å¸ƒã€åŠŸèƒ½æ›´æ–°ã€ç”¨æˆ·ä½“éªŒä¼˜åŒ–ä¸å¸‚åœºåŒ–ç­–ç•¥è¿½è¸ªæŠ¥é“",
        "icon": "M13 10V3L4 14h7v7l9-11h-7z"
    }
}

def fetch_rss(url, name):
    """æŠ“å–RSS"""
    try:
        print(f"Fetching RSS: {name}")
        feed = feedparser.parse(url)
        entries = []
        for entry in feed.entries[:5]:
            summary = entry.get('summary', entry.get('description', ''))
            clean_summary = re.sub(r'<[^>]+>', '', summary)
            
            # å¼ºåˆ¶ç¿»è¯‘
            title_cn = force_translate(entry.title)
            summary_cn = force_translate(clean_summary[:200])
            
            entries.append({
                "title": title_cn,
                "link": entry.link,
                "date": entry.get('published', ''),
                "summary": summary_cn if summary_cn else title_cn,
                "source": name,
                "content": f"<p>{summary_cn}</p><p><a href='{entry.link}' target='_blank'>æŸ¥çœ‹åŸæ–‡ï¼š{name}</a></p>"
            })
        print(f"  âœ“ {name}: {len(entries)} articles")
        return entries
    except Exception as e:
        print(f"  âœ— Error {name}: {e}")
        return []

def fetch_hf_papers():
    """æŠ“å–Hugging Faceè®ºæ–‡"""
    try:
        print("Fetching: Hugging Face Papers")
        resp = requests.get("https://huggingface.co/api/papers?limit=10", timeout=15)
        papers = resp.json()
        entries = []
        for p in papers:
            title = p.get('title', '')
            summary = p.get('summary', '')
            paper_id = p.get('id', '')
            
            if title:
                entries.append({
                    "title": f"[è®ºæ–‡] {force_translate(title)}",
                    "link": f"https://huggingface.co/papers/{paper_id}",
                    "date": p.get('publishedAt', ''),
                    "summary": force_translate(summary[:200]) if summary else "æœ€æ–°AIç ”ç©¶è®ºæ–‡",
                    "source": "Hugging Face",
                    "content": f"<p>{force_translate(summary[:200])}</p><p><a href='https://huggingface.co/papers/{paper_id}' target='_blank'>æŸ¥çœ‹è®ºæ–‡</a></p>"
                })
        print(f"  âœ“ Hugging Face: {len(entries)} papers")
        return entries
    except Exception as e:
        print(f"  âœ— Error HF: {e}")
        return []

def fetch_html_list(url, name, selector):
    """æŠ“å–HTMLåˆ—è¡¨"""
    try:
        print(f"Fetching HTML: {name}")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, 'html.parser')
        entries = []
        
        links = soup.select(selector)[:3]
        for link in links:
            href = link.get('href', '')
            if href and not href.startswith('http'):
                href = urljoin(url, href)
            title = link.get_text(strip=True)
            if title and 10 < len(title) < 150:
                entries.append({
                    "title": force_translate(title),
                    "link": href,
                    "date": datetime.now().isoformat(),
                    "summary": f"{name}æœ€æ–°åŠ¨æ€",
                    "source": name,
                    "content": f"<p>{name}å‘å¸ƒæ›´æ–°</p><p><a href='{href}' target='_blank'>æŸ¥çœ‹åŸæ–‡</a></p>"
                })
        print(f"  âœ“ {name}: {len(entries)} articles")
        return entries
    except Exception as e:
        print(f"  âœ— Error {name}: {e}")
        return []

def estimate_read_time(text):
    words = len(text) / 2
    minutes = max(1, round(words / 300))
    return f"{minutes} åˆ†é’Ÿ"

def build_content_database():
    """æ„å»ºæ•°æ®åº“ - å¼ºåˆ¶å¤šæ ·æ€§"""
    print("\n" + "="*50)
    print("å¼€å§‹æŠ“å–æ•°æ®...")
    print("="*50)
    
    database = {"summaries": [[]], "categories": {}}
    all_articles_by_source = {}  # æŒ‰æ¥æºåˆ†ç»„
    
    # æŠ“å–æ‰€æœ‰æ•°æ®
    for cat_key, sources in SOURCES.items():
        for source in sources:
            try:
                if source['type'] == 'rss':
                    entries = fetch_rss(source['url'], source['name'])
                elif source['type'] == 'hf_api':
                    entries = fetch_hf_papers()
                elif source['type'] == 'html':
                    entries = fetch_html_list(source['url'], source['name'], source['selector'])
                else:
                    entries = []
                
                # æŒ‰æ¥æºå­˜å‚¨ï¼Œç”¨äºåç»­å¤šæ ·æ€§é€‰æ‹©
                if entries:
                    all_articles_by_source[source['name']] = entries
                
                # åŒæ—¶æŒ‰åˆ†ç±»å­˜å‚¨
                if cat_key not in database["categories"]:
                    database["categories"][cat_key] = []
                database["categories"][cat_key].extend(entries)
                
            except Exception as e:
                print(f"Error processing {source['name']}: {e}")
    
    # å¤„ç†åˆ†ç±»æ•°æ® - å»é‡å¹¶é™åˆ¶æ•°é‡
    for cat_key in CATEGORY_META.keys():
        articles = database["categories"].get(cat_key, [])
        seen = set()
        unique = []
        for a in articles:
            if a['link'] not in seen:
                seen.add(a['link'])
                a['readTime'] = estimate_read_time(a.get('summary', ''))
                unique.append(a)
        
        database["categories"][cat_key] = {
            "title": CATEGORY_META[cat_key]["title"],
            "desc": CATEGORY_META[cat_key]["desc"],
            "icon": CATEGORY_META[cat_key]["icon"],
            "articles": unique[:8]
        }
        print(f"\n{cat_key}: {len(unique[:8])} articles")
    
    # å¼ºåˆ¶å¤šæ ·æ€§ï¼šä»ä¸åŒæ¥æºé€‰æ‘˜è¦ï¼Œæ¯ä¸ªæ¥æºæœ€å¤š1æ¡
    print("\n" + "="*50)
    print("é€‰æ‹©æ‘˜è¦ï¼ˆå¼ºåˆ¶å¤šæ ·æ€§ï¼‰...")
    
    summaries = []
    sources_used = set()
    
    # ä¼˜å…ˆçº§é¡ºåº
    priority_order = [
        "OpenAI Blog", "Anthropic News", "Google DeepMind", "Meta AI Blog",
        "Hugging Face", "NVIDIA Blog AI", "TechCrunch AI", "MIT Tech Review AI"
    ]
    
    # ç¬¬ä¸€è½®ï¼šæ¯ä¸ªä¼˜å…ˆçº§æ¥æºå–1æ¡
    for src_name in priority_order:
        if src_name in all_articles_by_source and all_articles_by_source[src_name]:
            article = all_articles_by_source[src_name][0]
            if article['link'] not in [s['url'] for s in summaries]:
                summaries.append({
                    "text": article['summary'][:120] + "..." if len(article['summary']) > 120 else article['summary'],
                    "source": article['source'],
                    "url": article['link']
                })
                sources_used.add(src_name)
                print(f"  âœ“ æ¥è‡ª {src_name}")
            if len(summaries) >= 3:
                break
    
    # å¦‚æœä¸å¤Ÿ3æ¡ï¼Œä»å…¶ä»–æ¥æºè¡¥å……
    if len(summaries) < 3:
        for src_name, articles in all_articles_by_source.items():
            if src_name not in sources_used and articles:
                article = articles[0]
                summaries.append({
                    "text": article['summary'][:120] + "..." if len(article['summary']) > 120 else article['summary'],
                    "source": article['source'],
                    "url": article['link']
                })
                print(f"  âœ“ æ¥è‡ª {src_name} (è¡¥å……)")
            if len(summaries) >= 3:
                break
    
    database["summaries"] = [summaries[:3]]
    
    print(f"\nâœ“ æ‘˜è¦é€‰æ‹©å®Œæˆ: {len(summaries)}æ¡ï¼Œæ¥è‡ª {len(sources_used)}ä¸ªä¸åŒæ¥æº")
    for i, s in enumerate(summaries, 1):
        print(f"  {i}. [{s['source']}] {s['text'][:40]}...")
    
    return database

def update_html_file():
    """æ›´æ–°HTML"""
    try:
        with open('index.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        new_db = build_content_database()
        json_str = json.dumps(new_db, ensure_ascii=False, indent=8)
        
        # æ›¿æ¢contentDatabase
        pattern = r'const contentDatabase = \{.*?\};'
        replacement = f'const contentDatabase = {json_str};'
        new_html = re.sub(pattern, replacement, html_content, flags=re.DOTALL)
        
        if new_html == html_content:
            # å¤‡é€‰æ›¿æ¢æ–¹æ¡ˆ
            lines = html_content.split('\n')
            for i, line in enumerate(lines):
                if 'const contentDatabase = {' in line:
                    start = i
                    brace = 1
                    j = i + 1
                    while j < len(lines) and brace > 0:
                        brace += lines[j].count('{') - lines[j].count('}')
                        if brace == 0:
                            new_lines = lines[:start] + [f'const contentDatabase = {json_str};'] + lines[j+1:]
                            new_html = '\n'.join(new_lines)
                            break
                        j += 1
                    break
        
        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(new_html)
        
        total = sum(len(c['articles']) for c in new_db['categories'].values())
        print(f"\nâœ… æ›´æ–°æˆåŠŸï¼æ€»æ–‡ç« æ•°: {total}, æ‘˜è¦æ•°: {len(new_db['summaries'][0])}")
        
    except Exception as e:
        print(f"Error: {e}")
        raise

if __name__ == '__main__':
    print("ğŸ¤– DATOU AI News - å¼ºåˆ¶ä¸­æ–‡ç¿»è¯‘ + æ¥æºå¤šæ ·æ€§")
    print("=" * 50)
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    update_html_file()
