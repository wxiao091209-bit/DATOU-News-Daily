#!/usr/bin/env python3
import os
import re
import json
import html
import random
from datetime import datetime, timedelta
from urllib.parse import urljoin
import feedparser
import requests
from bs4 import BeautifulSoup

# ç®€å•AIæœ¯è¯­ç¿»è¯‘å­—å…¸ï¼ˆå¸¸è§æ ‡é¢˜å…³é”®è¯æ˜ å°„ï¼‰
TRANSLATION_MAP = {
    # å…¬å¸/äº§å“
    "openai": "OpenAI",
    "anthropic": "Anthropic",
    "claude": "Claude",
    "gpt": "GPT",
    "gemini": "Gemini",
    "google": "è°·æ­Œ",
    "meta": "Meta",
    "nvidia": "è‹±ä¼Ÿè¾¾",
    "microsoft": "å¾®è½¯",
    "amazon": "äºšé©¬é€Š",
    
    # æŠ€æœ¯æœ¯è¯­
    "artificial intelligence": "äººå·¥æ™ºèƒ½",
    "machine learning": "æœºå™¨å­¦ä¹ ",
    "large language model": "å¤§è¯­è¨€æ¨¡å‹",
    "llm": "å¤§æ¨¡å‹",
    "agent": "æ™ºèƒ½ä½“",
    "ai agent": "AIæ™ºèƒ½ä½“",
    "coding": "ç¼–ç¨‹",
    "app": "åº”ç”¨",
    "model": "æ¨¡å‹",
    "training": "è®­ç»ƒ",
    "inference": "æ¨ç†",
    "chip": "èŠ¯ç‰‡",
    "gpu": "GPU",
    "robotics": "æœºå™¨äºº",
    "funding": "èèµ„",
    "investment": "æŠ•èµ„",
    "partnership": "åˆä½œ",
    "announcing": "å‘å¸ƒ",
    "introducing": "æ¨å‡º",
    "new": "å…¨æ–°",
    "update": "æ›´æ–°",
    "release": "å‘å¸ƒ",
    "launch": "ä¸Šçº¿",
    "available": "å¯ç”¨",
    "enterprise": "ä¼ä¸šçº§",
    "research": "ç ”ç©¶",
    "paper": "è®ºæ–‡",
    "benchmark": "åŸºå‡†æµ‹è¯•",
    "performance": "æ€§èƒ½",
    "multimodal": "å¤šæ¨¡æ€",
    "reasoning": "æ¨ç†èƒ½åŠ›",
    "alignment": "å¯¹é½",
    "safety": "å®‰å…¨",
    "open source": "å¼€æº",
}

def translate_title(title):
    """ç®€å•ç¿»è¯‘æ ‡é¢˜ï¼ˆå…³é”®è¯æ›¿æ¢+æ ¼å¼ä¼˜åŒ–ï¼‰"""
    if not title:
        return "æ— æ ‡é¢˜"
    
    # å¦‚æœå·²ç»æ˜¯ä¸­æ–‡ä¸ºä¸»ï¼Œç›´æ¥è¿”å›
    chinese_chars = len([c for c in title if '\u4e00' <= c <= '\u9fff'])
    if chinese_chars > len(title) * 0.3:
        return title
    
    # è‹±æ–‡æ ‡é¢˜ç¿»è¯‘å¤„ç†
    translated = title.lower()
    
    # æŒ‰é•¿åº¦é™åºæ›¿æ¢ï¼ˆé¿å…çŸ­è¯è¦†ç›–é•¿è¯ï¼‰
    for en, cn in sorted(TRANSLATION_MAP.items(), key=lambda x: len(x[0]), reverse=True):
        translated = translated.replace(en.lower(), cn)
    
    # é¦–å­—æ¯å¤§å†™
    translated = translated.capitalize()
    
    # å¦‚æœç¿»è¯‘åè¿˜æ˜¯å¤ªåƒè‹±æ–‡ï¼Œæ·»åŠ å‰ç¼€æç¤º
    if len([c for c in translated if '\u4e00' <= c <= '\u9fff']) < 3:
        return f"[æµ·å¤–] {title}"
    
    return translated

def translate_summary(text, source):
    """ç¿»è¯‘æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯"""
    if not text:
        return f"æ¥è‡ª{source}çš„æœ€æ–°åŠ¨æ€..."
    
    # æ¸…ç†HTML
    clean = re.sub(r'<[^>]+>', '', text)
    
    # å¦‚æœå·²æœ‰ä¸­æ–‡ï¼Œç›´æ¥æˆªæ–­
    chinese_chars = len([c for c in clean if '\u4e00' <= c <= '\u9fff'])
    if chinese_chars > len(clean) * 0.2:
        return clean[:120] + "..." if len(clean) > 120 else clean
    
    # ç¿»è¯‘å…³é”®æœ¯è¯­
    translated = clean.lower()
    for en, cn in sorted(TRANSLATION_MAP.items(), key=lambda x: len(x[0]), reverse=True):
        translated = translated.replace(en.lower(), cn)
    
    # æ·»åŠ æ¥æºæç¤º
    result = translated.capitalize()
    if len(result) > 150:
        result = result[:150] + "..."
    
    return result

# ä¸€æ‰‹ä¿¡æºé…ç½®ï¼ˆå®˜æ–¹RSSæºï¼Œå»é™¤æ‰€æœ‰æ¸¯æ¾³å°ç«™ç‚¹ï¼‰
SOURCES = {
    "bigModel": [  # å¤§æ¨¡å‹
        {"name": "OpenAI Blog", "url": "https://openai.com/blog/rss.xml", "type": "rss"},
        {"name": "Anthropic News", "url": "https://www.anthropic.com/news/rss.xml", "type": "rss"},
        {"name": "Google DeepMind", "url": "https://deepmind.google/discover/feed/", "type": "rss"},
        {"name": "Meta AI Blog", "url": "https://ai.meta.com/blog/rss/", "type": "rss"},
        {"name": "Hugging Face Papers", "url": "https://huggingface.co/api/papers?limit=15", "type": "hf_api"},
        {"name": "Mistral AI", "url": "https://mistral.ai/news", "type": "html", "selector": "article h2 a, article h3 a"},
        {"name": "xAI", "url": "https://x.ai/news", "type": "html", "selector": "a[href*='/news/']"},
    ],
    "hardware": [  # AIç¡¬ä»¶
        {"name": "NVIDIA Blog AI", "url": "https://blogs.nvidia.com/blog/category/artificial-intelligence/feed/", "type": "rss"},
        {"name": "NVIDIA Blog Robotics", "url": "https://blogs.nvidia.com/blog/category/robotics/feed/", "type": "rss"},
    ],
    "investment": [  # æŠ•èèµ„
        {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed/", "type": "rss"},
        {"name": "TechCrunch Funding", "url": "https://techcrunch.com/category/venture/feed/", "type": "rss"},
        {"name": "VentureBeat AI", "url": "https://venturebeat.com/category/ai/feed/", "type": "rss"},
    ],
    "global": [  # å‡ºæµ·åŠ¨æ€
        {"name": "MIT Tech Review AI", "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed", "type": "rss"},
        {"name": "The Verge AI", "url": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml", "type": "rss"},
    ],
    "industry": [  # äº§ä¸šè§‚å¯Ÿ
        {"name": "Google AI Blog", "url": "https://ai.googleblog.com/feeds/posts/default", "type": "rss"},
        {"name": "Microsoft AI Blog", "url": "https://blogs.microsoft.com/ai/feed/", "type": "rss"},
    ],
    "product": [  # äº§å“å¿«è®¯
        {"name": "OpenAI Product", "url": "https://openai.com/blog/rss.xml", "type": "rss"},
        {"name": "Anthropic Product", "url": "https://www.anthropic.com/news/rss.xml", "type": "rss"},
    ]
}

# åˆ†ç±»å…ƒæ•°æ®ï¼ˆä¸ç°æœ‰HTMLç»“æ„å®Œå…¨åŒ¹é…ï¼‰
CATEGORY_META = {
    "bigModel": {
        "title": "å¤§æ¨¡å‹",
        "desc": "GPT-5ã€Claude 4ã€Gemini Ultra ç­‰å‰æ²¿å¤§æ¨¡å‹æŠ€æœ¯çªç ´ä¸å•†ä¸šåŒ–è¿›å±•è¿½è¸ª",
        "icon": "M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"
    },
    "hardware": {
        "title": "AI ç¡¬ä»¶",
        "desc": "ç®—åŠ›èŠ¯ç‰‡ã€æœºå™¨äººã€ç«¯ä¾§è®¾å¤‡ã€AI Phone ç­‰ç¡¬ä»¶è½½ä½“æŠ€æœ¯é©æ–°ä¸äº§ä¸šé“¾åŠ¨å‘",
        "icon": "M9 3v2m6-2v2M9 19v2m6-2v2M5 9H3m2 6H3m18-6h-2m2 6h-2M7 19h10a2 2 0 002-2V7a2 2 0 00-2-2H7a2 2 0 00-2 2v10a2 2 0 002 2zM9 9h6v6H9V9z"
    },
    "global": {
        "title": "å‡ºæµ·åŠ¨æ€",
        "desc": "ä¸­å›½ AI ä¼ä¸šå…¨çƒåŒ–å¸ƒå±€ã€æµ·å¤–ç›‘ç®¡æ”¿ç­–ã€è·¨å¢ƒæŠ•èèµ„ä¸æœ¬åœ°åŒ–æˆ˜ç•¥åˆ†æ",
        "icon": "M3.055 11H5a2 2 0 012 2v1a2 2 0 002 2 2 2 0 012 2v2.945M8 3.935V5.5A2.5 2.5 0 0010.5 8h.5a2 2 0 012 2 2 2 0 104 0 2 2 0 012-2h1.064M15 20.488V18a2 2 0 012-2h3.064M21 12a9 9 0 11-18 0 9 9 0 0118 0z"
    },
    "investment": {
        "title": "æŠ•èèµ„",
        "desc": "ä¸€çº§å¸‚åœºèèµ„é€Ÿé€’ã€ç‹¬è§’å…½ä¼°å€¼å˜åŠ¨ã€IPO åŠ¨æ€ä¸èµ„æœ¬é£å‘è§£è¯»",
        "icon": "M12 8c-1.657 0-3 .895-3 2s1.343 2 3 2 3 .895 3 2-1.343 2-3 2m0-8c1.11 0 2.08.402 2.599 1M12 8V7m0 1v8m0 0v1m0-1c-1.11 0-2.08-.402-2.599-1M21 12a9 9 0 11-18 0 9 9 0 0118 0z"
    },
    "industry": {
        "title": "äº§ä¸šè§‚å¯Ÿ",
        "desc": "è¡Œä¸šæ”¿ç­–è§£è¯»ã€ç«äº‰æ ¼å±€åˆ†æã€æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹ä¸å•†ä¸šæ¨¡å¼æ¼”è¿›ç ”ç©¶",
        "icon": "M19 21V5a2 2 0 00-2-2H7a2 2 0 00-2 2v16m14 0h2m-2 0h-5m-9 0H3m2 0h5M9 7h1m-1 4h1m4-4h1m-1 4h1m-5 10v-5a1 1 0 011-1h2a1 1 0 011 1v5m-4 0h4"
    },
    "product": {
        "title": "äº§å“å¿«è®¯",
        "desc": "AI åº”ç”¨æ–°å“å‘å¸ƒã€åŠŸèƒ½æ›´æ–°ã€ç”¨æˆ·ä½“éªŒä¼˜åŒ–ä¸å¸‚åœºåŒ–ç­–ç•¥è¿½è¸ªæŠ¥é“",
        "icon": "M13 10V3L4 14h7v7l9-11h-7z"
    }
}

def fetch_rss(url, name):
    """æŠ“å–RSS feed"""
    try:
        print(f"Fetching: {name}")
        feed = feedparser.parse(url)
        entries = []
        for entry in feed.entries[:8]:
            published = entry.get('published', entry.get('updated', ''))
            summary = entry.get('summary', entry.get('description', ''))
            clean_summary = re.sub(r'<[^>]+>', '', summary)
            
            entries.append({
                "title": translate_title(entry.title),  # ç¿»è¯‘æ ‡é¢˜
                "link": entry.link,
                "date": published,
                "summary": translate_summary(clean_summary, name),  # ç¿»è¯‘æ‘˜è¦
                "source": name,
                "content": f"<p>{translate_summary(clean_summary, name)}</p><p><a href='{entry.link}' target='_blank'>æŸ¥çœ‹åŸæ–‡ï¼š{name}</a></p>"
            })
        return entries
    except Exception as e:
        print(f"Error {name}: {e}")
        return []

def fetch_hf_papers():
    """æŠ“å–Hugging Faceæœ€æ–°è®ºæ–‡"""
    try:
        print("Fetching: Hugging Face Papers")
        resp = requests.get("https://huggingface.co/api/papers?limit=15", timeout=15)
        papers = resp.json()
        entries = []
        for p in papers:
            paper_id = p.get('id', '')
            title = p.get('title', '')
            summary = p.get('summary', '')
            
            if title and len(title) > 10:
                entries.append({
                    "title": f"[è®ºæ–‡] {translate_title(title)}",
                    "link": f"https://huggingface.co/papers/{paper_id}",
                    "date": p.get('publishedAt', ''),
                    "summary": translate_summary(summary, "Hugging Face"),
                    "source": "Hugging Face",
                    "content": f"<p>è®ºæ–‡æ‘˜è¦ï¼š{translate_summary(summary, 'Hugging Face')}</p><p><a href='https://huggingface.co/papers/{paper_id}' target='_blank'>æŸ¥çœ‹è®ºæ–‡è¯¦æƒ…</a></p>"
                })
        return entries
    except Exception as e:
        print(f"Error HF: {e}")
        return []

def fetch_html_list(url, name, selector):
    """ç®€å•çš„HTMLåˆ—è¡¨æŠ“å–"""
    try:
        print(f"Fetching: {name}")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.content, 'html.parser')
        entries = []
        
        links = soup.select(selector)[:5]
        for link in links:
            href = link.get('href', '')
            if href and not href.startswith('http'):
                href = urljoin(url, href)
            title = link.get_text(strip=True)
            if title and 15 < len(title) < 200:
                entries.append({
                    "title": translate_title(title),
                    "link": href,
                    "date": datetime.now().isoformat(),
                    "summary": f"{name}æœ€æ–°åŠ¨æ€...",
                    "source": name,
                    "content": f"<p>{name}å‘å¸ƒäº†æœ€æ–°æ›´æ–°ã€‚</p><p><a href='{href}' target='_blank'>æŸ¥çœ‹åŸæ–‡ï¼š{name}</a></p>"
                })
        return entries
    except Exception as e:
        print(f"Error {name}: {e}")
        return []

def estimate_read_time(text):
    """ä¼°ç®—é˜…è¯»æ—¶é—´"""
    words = len(text) / 2
    minutes = max(1, round(words / 300))
    return f"{minutes} åˆ†é’Ÿ"

def build_content_database():
    """æ„å»ºå†…å®¹æ•°æ®åº“"""
    database = {"summaries": [], "categories": {}}  # ç¡®ä¿åˆå§‹åŒ–ä¸ºç©ºå­—å…¸
    
    all_articles = []
    category_articles = {key: [] for key in CATEGORY_META.keys()}
    
    for cat_key, sources in SOURCES.items():
        print(f"\nProcessing: {cat_key}")
        for source in sources:
            try:
                if source['type'] == 'rss':
                    entries = fetch_rss(source['url'], source['name'])
                elif source['type'] == 'hf_api':
                    entries = fetch_hf_papers()
                elif source['type'] == 'html':
                    entries = fetch_html_list(source['url'], source['name'], source['selector'])
                else:
                    entries = []
                
                for entry in entries:
                    entry['readTime'] = estimate_read_time(entry.get('summary', ''))
                    final_cat = cat_key
                    
                    # æ™ºèƒ½åˆ†ç±»
                    title_lower = entry['title'].lower()
                    if any(k in title_lower for k in ['èèµ„', 'æŠ•èµ„', 'funding', 'investment', '$', 'million', 'ä¼°å€¼', 'billion']):
                        final_cat = 'investment'
                    elif any(k in title_lower for k in ['å‘å¸ƒ', 'æ¨å‡º', 'ä¸Šçº¿', 'launch', 'release', 'product', 'update', 'available']):
                        if final_cat not in ['product', 'bigModel']:
                            final_cat = 'product'
                    
                    if final_cat in category_articles:
                        category_articles[final_cat].append(entry)
                    all_articles.append(entry)
                    
            except Exception as e:
                print(f"Error: {e}")
    
    # å¡«å……åˆ†ç±»æ•°æ® - ç¡®ä¿æ¯ä¸ªåˆ†ç±»éƒ½æœ‰æ•°æ®ï¼ˆå³ä½¿ä¸ºç©ºæ•°ç»„ï¼‰
    for cat_key, meta in CATEGORY_META.items():
        articles = category_articles.get(cat_key, [])
        # å»é‡
        seen = set()
        unique = []
        for a in articles:
            if a['link'] not in seen:
                seen.add(a['link'])
                unique.append(a)
        
        database["categories"][cat_key] = {
            "title": meta["title"],
            "desc": meta["desc"],
            "icon": meta["icon"],
            "articles": unique[:8]  # æœ€å¤š8æ¡
        }
        print(f"{cat_key}: {len(unique[:8])} articles")
    
    # ç”Ÿæˆä»Šæ—¥æ ¸å¿ƒæ‘˜è¦ - ç¡®ä¿å¤šæ ·æ€§ï¼ˆä¸è¦å…¨æ˜¯OpenAIï¼‰
    # æŒ‰æ¥æºåˆ†ç»„ï¼Œæ¯ä¸ªæ¥æºå–1æ¡ï¼Œç¡®ä¿å¤šæ ·æ€§
    source_groups = {}
    for a in all_articles:
        src = a['source']
        if src not in source_groups:
            source_groups[src] = []
        source_groups[src].append(a)
    
    # ä¼˜å…ˆå–ä¸åŒæ¥æºçš„Topæ–‡ç« 
    summaries = []
    priority_sources = ['OpenAI Blog', 'Anthropic News', 'Google DeepMind', 'Meta AI Blog', 'Hugging Face', 'NVIDIA Blog AI']
    
    for src in priority_sources:
        if src in source_groups and source_groups[src]:
            summaries.append(source_groups[src][0])
        if len(summaries) >= 3:
            break
    
    # å¦‚æœä¸å¤Ÿ3æ¡ï¼Œè¡¥å……å…¶ä»–æ¥æº
    if len(summaries) < 3:
        for src, articles in source_groups.items():
            if articles and articles[0] not in summaries:
                summaries.append(articles[0])
            if len(summaries) >= 3:
                break
    
    # æ ¼å¼åŒ–æ‘˜è¦
    formatted = []
    for article in summaries[:3]:
        clean = re.sub(r'<[^>]+>', '', article.get('summary', ''))
        if len(clean) > 150:
            clean = clean[:150] + "..."
        
        formatted.append({
            "text": clean or article['title'],
            "source": article['source'],
            "url": article['link']
        })
    
    database["summaries"] = [formatted]  # ä¿æŒæ•°ç»„çš„æ•°ç»„ç»“æ„
    
    # å…³é”®ï¼šç¡®ä¿categoriesä¸ä¸ºç©º
    if not database["categories"]:
        print("WARNING: categories is empty!")
        # å¡«å……ç©ºç»“æ„é¿å…æŠ¥é”™
        for cat_key, meta in CATEGORY_META.items():
            database["categories"][cat_key] = {
                "title": meta["title"],
                "desc": meta["desc"],
                "icon": meta["icon"],
                "articles": []
            }
    
    return database

def update_html_file():
    """æ›´æ–°HTMLæ–‡ä»¶"""
    try:
        with open('index.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        new_db = build_content_database()
        
        # éªŒè¯æ•°æ®ç»“æ„
        print(f"\nDatabase structure check:")
        print(f"- summaries count: {len(new_db['summaries'])}")
        print(f"- summaries[0] count: {len(new_db['summaries'][0]) if new_db['summaries'] else 0}")
        print(f"- categories count: {len(new_db['categories'])}")
        for cat, data in new_db['categories'].items():
            print(f"  - {cat}: {len(data.get('articles', []))} articles")
        
        json_str = json.dumps(new_db, ensure_ascii=False, indent=8)
        
        # æ›¿æ¢contentDatabase
        pattern = r'const contentDatabase = \{.*?\};'
        replacement = f'const contentDatabase = {json_str};'
        new_html = re.sub(pattern, replacement, html_content, flags=re.DOTALL)
        
        if new_html == html_content:
            print("Warning: Using line-based replacement...")
            lines = html_content.split('\n')
            for i, line in enumerate(lines):
                if 'const contentDatabase = {' in line:
                    start = i
                    brace = 1
                    j = i + 1
                    while j < len(lines) and brace > 0:
                        brace += lines[j].count('{') - lines[j].count('}')
                        if brace == 0:
                            new_lines = lines[:start] + [f'const contentDatabase = {json_str};'] + lines[j+1:]
                            new_html = '\n'.join(new_lines)
                            break
                        j += 1
                    break
        
        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(new_html)
        
        total = sum(len(c.get('articles', [])) for c in new_db['categories'].values())
        print(f"\nâœ… Success! Total articles: {total}, Summaries: {len(new_db['summaries'][0]) if new_db['summaries'] else 0}")
        
    except Exception as e:
        print(f"Error: {e}")
        raise

if __name__ == '__main__':
    print("ğŸ¤– DATOU AI News - Official Sources + Chinese Translation")
    print("=" * 50)
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("Sources: OpenAI, Anthropic, DeepMind, Meta, NVIDIA, HF")
    print("=" * 50)
    update_html_file()
